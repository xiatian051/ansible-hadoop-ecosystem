<configuration>
        <!-- nodemanager 配置 ================================================= -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>
        <property>
                <description>Address where the localizer IPC is.</description>
                <name>yarn.nodemanager.localizer.address</name>
                <value>0.0.0.0:23344</value>
        </property>
        <property>
                <description>NM Webapp address.</description>
                <name>yarn.nodemanager.webapp.address</name>
                <value>0.0.0.0:23999</value>
        </property>

        <!-- HA 配置 =============================================================== -->
        <!-- Resource Manager Configs -->
        <property>
                <name>yarn.resourcemanager.connect.retry-interval.ms</name>
                <value>2000</value>
        </property>
        <property>
                <name>yarn.resourcemanager.ha.enabled</name>
                <value>true</value>
        </property>
        <property>
                <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
        <!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing -->
        <property>
                <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
                <value>true</value>
        </property>
        <!-- 集群名称，确保HA选举时对应的集群 -->
        <property>
                <name>yarn.resourcemanager.cluster-id</name>
                <value>yarn-cluster</value>
        </property>
        <property>
                <name>yarn.resourcemanager.ha.rm-ids</name>
                <value>rm1,rm2</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
                <value>5000</value>
        </property>
        <!-- ZKRMStateStore 配置 -->
        <property>
                <name>yarn.resourcemanager.store.class</name>
                <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        </property>
        <property>
                <name>yarn.resourcemanager.zk-address</name>
                <value>{% for host in groups['zookeepers'] %}{{ hostvars[host].ipv4_address }}:2181{% if not loop.last %},{% endif %}{% endfor %}</value>
        </property>
        <property>
                <name>yarn.resourcemanager.zk.state-store.address</name>
                <value>{% for host in groups['zookeepers'] %}{{ hostvars[host].ipv4_address }}:2181{% if not loop.last %},{% endif %}{% endfor %}</value>
        </property>
        <!-- Client访问RM的RPC地址 (applications manager interface) -->
        {% for host in groups['resourceManagers'] %}
    	<property>
        	<name>yarn.resourcemanager.address.rm{{ loop.index }}</name>
        	<value>{{ hostvars[host].ipv4_address }}:23140</value>
    	</property>
        <!-- AM访问RM的RPC地址(scheduler interface) -->
    	<property>
        	<name>yarn.resourcemanager.scheduler.address.rm{{ loop.index }}</name>
        	<value>{{ hostvars[host].ipv4_address }}:23130</value>
    	</property>
        <!-- RM admin interface -->
    	<property>
        	<name>yarn.resourcemanager.admin.address.rm{{ loop.index }}</name>
        	<value>{{ hostvars[host].ipv4_address }}:23141</value>
    	</property>
        <!--NM访问RM的RPC端口 -->
        <property>
                <name>yarn.resourcemanager.resource-tracker.address.rm{{ loop.index }}</name>
                <value>{{ hostvars[host].ipv4_address }}:23125</value>
        </property>
        <!-- RM web application 地址 -->
        <property>
                <name>yarn.resourcemanager.webapp.address.rm{{ loop.index }}</name>
                <value>{{ hostvars[host].ipv4_address }}:23188</value>
        </property>
	<property>
                <name>yarn.resourcemanager.webapp.https.address.rm{{ loop.index }}</name>
                <value>{{ hostvars[host].ipv4_address }}:23189</value>
        </property>
        {% endfor %}

        <!--- for you -->

        <!-- scheduler -->
        <property>
                <name>yarn.resourcemanager.scheduler.class</name>
                <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
        </property>
        <property>
                <name>yarn.scheduler.fair.allocation.file</name>
                <value>{{ service_dir}}/hadoop-{{ hadoop_version}}/etc/hadoop/fair-scheduler.xml</value>
        </property>
        <property>
                <name>yarn.scheduler.fair.sizebasedweight</name>
                <value>false</value>
        </property>
        <property>
                <name>yarn.scheduler.fair.assignmultiple</name>
                <value>false</value>
        </property>

        <property>
                <name>yarn.scheduler.fair.max.assign</name>
                <value>-1</value>
        </property>

        <property>
                <description>The minimum allocation for every container request at the RM,
                        in MBs. Memory requests lower than this won't take effect,
                        and the specified value will get allocated at minimum.</description>
                <name>yarn.scheduler.minimum-allocation-mb</name>
                <value>512</value>
        </property>

        <property>
                <description>The maximum allocation for every container request at the RM,
                        in MBs. Memory requests higher than this won't take effect,
                        and will get capped to this value.</description>
                <name>yarn.scheduler.maximum-allocation-mb</name>
                <value>8192</value>
        </property>

        <property>
                <description>The minimum allocation for every container request at the RM,
                        in terms of virtual CPU cores. Requests lower than this won't take effect,
                        and the specified value will get allocated the minimum.</description>
                <name>yarn.scheduler.minimum-allocation-vcores</name>
                <value>1</value>
        </property>

        <property>
                <description>The maximum allocation for every container request at the RM,
                        in terms of virtual CPU cores. Requests higher than this won't take effect,
                        and will get capped to this value.</description>
                <name>yarn.scheduler.maximum-allocation-vcores</name>
                <value>6</value>
        </property>
        <property>
                <name>yarn.scheduler.increment-allocation-mb</name>
                <value>512</value>
        </property>

        <property>
                <name>yarn.scheduler.increment-allocation-vcores</name>
                <value>1</value>
        </property>


        <property>
                <description>Amount of physical memory, in MB, that can be allocated
                        for containers.</description>
                <name>yarn.nodemanager.resource.memory-mb</name>
                <value>200480</value>
        </property>

        <property>
                <description>Number of CPU cores that can be allocated
                        for containers.</description>
                <name>yarn.nodemanager.resource.cpu-vcores</name>
                <value>12</value>
        </property>

        <!-- scheduler end -->


        <property>
                <description>Path to file with nodes to exclude.</description>
                <name>yarn.resourcemanager.nodes.exclude-path</name>
                <value>{{ service_dir }}/hadoop-{{ hadoop_version }}/etc/hadoop/yarn-excludes</value>
        </property>

        <!--
        <property>
          <name>yarn.app.mapreduce.am.job.recovery.enable</name>
          <value>false</value>
        </property>
        -->
        <property>
                <name>yarn.nodemanager.local-dirs</name>
                <value>{{ hadoop['dfs_dir_parent'] }}{{ hadoop['dfs_dir_tmp'] }}</value> 
        </property>
        <property>
                <name>yarn.nodemanager.log-dirs</name>
                <value>{{ service_dir }}/hadoop-{{ hadoop_version }}/logs</value>
        </property>

        <!-- log config  start-->
        <property>
                <description>Whether to enable log aggregation</description>
                <name>yarn.log-aggregation-enable</name>
                <value>true</value>
        </property>

        <property>
                <description>How long to keep aggregation logs before deleting them.  -1 disables.
                        Be careful set this too small and you will spam the name node.</description>
                <name>yarn.log-aggregation.retain-seconds</name>
                <value>604800</value>
        </property>

        <property>
                <description>How long to wait between aggregated log retention checks.
                        If set to 0 or a negative value then the value is computed as one-tenth
                        of the aggregated log retention time. Be careful set this too small and
                        you will spam the name node.</description>
                <name>yarn.log-aggregation.retain-check-interval-seconds</name>
                <value>-1</value>
        </property>

        <property>
                <description>Time in seconds to retain user logs. Only applicable if
                        log aggregation is disabled
                </description>
                <name>yarn.nodemanager.log.retain-seconds</name>
                <value>172800</value>
        </property>

        <property>
                <description>Where to aggregate logs to.</description>
                <name>yarn.nodemanager.remote-app-log-dir</name>
                <value>/tmp/logs</value>
        </property>

        <property>
                <description>The remote log dir will be created at
                        {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}
                </description>
                <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
                <value>logs</value>
        </property>

        <!--
        <property>
          <description>T-file compression types used to compress aggregated logs.</description>
          <name>yarn.nodemanager.log-aggregation.compression-type</name>
          <value>gz</value>
        </property>
        -->

        <property>
                <name>yarn.log.server.url</name>
                <value>http://{% for host in groups['jobhistoryNodes'] %}{{ hostvars[host].ipv4_address }}{% if not loop.last %},{% endif %}{% endfor %}:19888/jobhistory/logs/</value>
                <description>URL for job history server</description>
        </property>

        <!-- app config -->
        <property>
                <name>yarn.application.classpath</name>
                <value>
                        $HADOOP_CONF_DIR,
                        $HADOOP_COMMON_HOME/share/hadoop/common/*,
                        $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
                        $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,
                        $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
                        $HADOOP_YARN_HOME/share/hadoop/yarn/*,
                        $HADOOP_YARN_HOME/share/hadoop/yarn/lib/*
                </value>
        </property>

        <!-- security config -->
        <property>
                <name>yarn.acl.enable</name>
                <value>false</value>
                <description>Enable ACLs? Defaults to false.</description>
        </property>

        <property>
                <name>yarn.admin.acl</name>
                <value>root</value>
                <description>ACL to set admins on the cluster. ACLs are of for comma-separated-usersspacecomma-separated-groups. Defaults to special value of * which means anyone. Special value of just space means no one has access</description>
        </property>

        <!-- proxy config -->
        <property>
                <description>The address for the web proxy as HOST:PORT, if this is not
                        given then the proxy will run as part of the RM</description>
                <name>yarn.web-proxy.address</name>
                <value>http://{% for host in groups['jobhistoryNodes'] %}{{ hostvars[host].ipv4_address }}{% if not loop.last %},{% endif %}{% endfor %}:8045</value>
        </property>

</configuration>
