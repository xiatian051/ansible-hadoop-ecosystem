<configuration>
        <!-- 配置 MapReduce Applications -->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>

        <property>
                <name>mapreduce.shuffle.port</name>
                <value>8281</value>
        </property>

        <property>
                <name>mapreduce.job.reduce.shuffle.consumer.plugin.class</name>
                <value>org.apache.hadoop.mapreduce.task.reduce.Shuffle</value>
                <description>
                        Name of the class whose instance will be used
                        to send shuffle requests by reducetasks of this job.
                        The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.
                </description>
        </property>

        <property>
                <name>mapreduce.job.counters.limit</name>
                <value>180</value>
                <description>Limit on the number of user counters allowed per job.
                </description>
        </property>

        <property>
                <name>mapreduce.job.jvm.numtasks</name>
                <value>-1</value>
                <description>How many tasks to run per jvm. If set to -1, there is
                        no limit.
                </description>
        </property>

        <property>
                <name>mapreduce.job.ubertask.enable</name>
                <value>true</value>
                <description>Whether to enable the small-jobs "ubertask" optimization,
                        which runs "sufficiently small" jobs sequentially within a single JVM.
                        "Small" is defined by the following maxmaps, maxreduces, and maxbytes
                        settings.  Users may override this value.
                </description>
        </property>

        <property>
                <name>mapreduce.job.ubertask.maxmaps</name>
                <value>9</value>
                <description>Threshold for number of maps, beyond which job is considered
                        too big for the ubertasking optimization.  Users may override this value,
                        but only downward.
                </description>
        </property>

        <property>
                <name>mapreduce.job.ubertask.maxreduces</name>
                <value>1</value>
                <description>Threshold for number of reduces, beyond which job is considered
                        too big for the ubertasking optimization.  CURRENTLY THE CODE CANNOT SUPPORT
                        MORE THAN ONE REDUCE and will ignore larger values.  (Zero is a valid max,
                        however.)  Users may override this value, but only downward.
                </description>
        </property>

        <property>
                <name>mapreduce.job.ubertask.maxbytes</name>
                <value>67108864</value>
                <description>Threshold for number of input bytes, beyond which job is
                        considered too big for the ubertasking optimization.  If no value is
                        specified, dfs.block.size is used as a default.  Be sure to specify a
                        default value in mapred-site.xml if the underlying filesystem is not HDFS.
                        Users may override this value, but only downward.
                </description>
        </property>

        <!--
        <property>
          <name>mapreduce.input.fileinputformat.split.minsize</name>
          <value>128000000</value>
          <description>The minimum size chunk that map input should be split
          into.  Note that some file formats may have minimum split sizes that
          take priority over this setting.</description>
        </property>
        -->

        <property>
                <name>mapreduce.task.io.sort.factor</name>
                <value>40</value>
                <description>The number of streams to merge at once while sorting
                        files.  This determines the number of open file handles.</description>
        </property>

        <property>
                <name>mapreduce.task.io.sort.mb</name>
                <value>1024</value>
                <description>The total amount of buffer memory to use while sorting
                        files, in megabytes.  By default, gives each merge stream 1MB, which
                        should minimize seeks.</description>
        </property>

        <!--
        <property>
          <name>mapred.child.java.opts</name>
          <value>-Xmx1024m</value>
          <description>Java opts for the task tracker child processes.
          The following symbol, if present, will be interpolated: @taskid@ is replaced
          by current TaskID. Any other occurrences of '@' will go unchanged.
          For example, to enable verbose gc logging to a file named for the taskid in
          /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
                -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc

          Usage of -Djava.library.path can cause programs to no longer function if
          hadoop native libraries are used. These values should instead be set as part
          of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and
          mapreduce.reduce.env config settings.
          </description>
        </property>
        -->
        <!--3072-->
        <property>
                <name>mapreduce.map.java.opts</name>
                <value>-server -Xms1240m -Xmx1240m -XX:NewSize=800m -XX:MaxNewSize=800m -XX:PermSize=32m -XX:MaxPermSize=32m -XX:ParallelGCThreads=2 -XX:+UseTLAB -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=68 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</value>
        </property>

        <property>
                <name>mapreduce.reduce.java.opts</name>
                <value>-server -Xms1024m -Xmx1024m -XX:NewSize=800 -XX:MaxNewSize=800m -XX:PermSize=32m -XX:MaxPermSize=32m -XX:+DisableExplicitGC -XX:ParallelGCThreads=4 -XX:+UseTLAB -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=68 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0</value>
                <!-- -XX:HeapDumpPath=/data1/ochadoop/loggc/taskid_@taskid@.hprof -XX:+PrintHeapAtGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -verbose:gc -Xloggc:/data1/ochadoop/loggc/red_@taskid@_gc.log -->
        </property>

        <property>
                <name>mapreduce.map.memory.mb</name>
                <value>1024</value>
                <description></description>
        </property>

        <property>
                <name>mapreduce.map.cpu.vcores</name>
                <value>1</value>
                <description>
                        The number of virtual cores required for each map task.
                </description>
        </property>

        <property>
                <name>mapreduce.reduce.memory.mb</name>
                <value>1024</value>
                <description></description>
        </property>

        <property>
                <name>mapreduce.reduce.cpu.vcores</name>
                <value>1</value>
                <description>
                        The number of virtual cores required for each reduce task.
                </description>
        </property>

        <!--
        <property>
          <name>mapreduce.job.reduces</name>
          <value>100</value>
          <description>The default number of reduce tasks per job. Typically set to 99%
          of the cluster's reduce capacity, so that if a node fails the reduces can
          still be executed in a single wave.
          Ignored when mapreduce.jobtracker.address is "local".
          </description>
        </property>
        -->

        <property>
                <name>mapreduce.job.reduce.slowstart.completedmaps</name>
                <value>0.95</value>
                <description>Fraction of the number of maps in the job which should be
                        complete before reduces are scheduled for the job.
                </description>
        </property>

        <property>
                <name>mapreduce.reduce.shuffle.parallelcopies</name>
                <value>50</value>
                <description>The default number of parallel transfers run by reduce
                        during the copy(shuffle) phase.
                </description>
        </property>

        <!-- no compress for DFSIO -->
        <property>
                <name>mapreduce.output.fileoutputformat.compress</name>
                <value>true</value>
                <description>Should the job outputs be compressed?
                </description>
        </property>

        <property>
                <name>mapreduce.output.fileoutputformat.compress.type</name>
                <value>BLOCK</value>
                <description>If the job outputs are to compressed as SequenceFiles, how should
                        they be compressed? Should be one of NONE, RECORD or BLOCK.
                </description>
        </property>

        <property>
                <name>mapreduce.output.fileoutputformat.compress.codec</name>
                <!-- <value>org.apache.hadoop.io.compress.GzipCodec</value> -->
                <value>org.apache.hadoop.io.compress.SnappyCodec</value>
                <description>If the job outputs are compressed, how should they be compressed?
                </description>
        </property>

        <property>
                <name>mapreduce.map.output.compress</name>
                <value>true</value>
                <description>Should the outputs of the maps be compressed before being
                        sent across the network. Uses SequenceFile compression.
                </description>
        </property>

        <property>
                <name>mapreduce.map.output.compress.codec</name>
                <value>org.apache.hadoop.io.compress.SnappyCodec</value>
                <!-- <value>org.apache.hadoop.io.compress.GzipCodec</value> -->
                <description>If the map outputs are compressed, how should they be
                        compressed?
                </description>
        </property>

        <property>
                <name>yarn.app.mapreduce.am.resource.mb</name>
                <value>1536</value>
                <description>The amount of memory the MR AppMaster needs.</description>
        </property>

        <property>
                <name>yarn.app.mapreduce.am.command-opts</name>
                <value>-Xmx1280m -Dhadoop.conf.dir=${HADOOP_CONF_DIR}</value>
                <description>Java opts for the MR App Master processes.
                        The following symbol, if present, will be interpolated: @taskid@ is replaced
                        by current TaskID. Any other occurrences of '@' will go unchanged.
                        For example, to enable verbose gc logging to a file named for the taskid in
                        /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
                        -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc

                        Usage of -Djava.library.path can cause programs to no longer function if
                        hadoop native libraries are used. These values should instead be set as part
                        of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and
                        mapreduce.reduce.env config settings.
                </description>
        </property>

        <property>
                <name>mapreduce.map.maxattempts</name>
                <value>6</value>
                <description>Expert: The maximum number of attempts per map task.
                        In other words, framework will try to execute a map task these many number
                        of times before giving up on it.
                </description>
        </property>

        <property>
                <name>mapreduce.reduce.maxattempts</name>
                <value>6</value>
                <description>Expert: The maximum number of attempts per reduce task.
                        In other words, framework will try to execute a reduce task these many number
                        of times before giving up on it.
                </description>
        </property>

        <property>
                <name>mapreduce.map.speculative</name>
                <value>false</value>
                <description>If true, then multiple instances of some map tasks
                        may be executed in parallel.</description>
        </property>

        <property>
                <name>mapreduce.reduce.speculative</name>
                <value>false</value>
                <description>If true, then multiple instances of some reduce tasks
                        may be executed in parallel.</description>
        </property>

        <property>
                <name>mapreduce.admin.user.env</name>
                <value>LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native:$HADOOP_COMMON_HOME/lib/native/Linux-amd64-64:$JAVA_HOME/jre/lib/amd64:$JAVA_HOME/jre/lib/amd64/server:/usr/lib64:/usr/local/lib:/usr/lib:$LD_LIBRARY_PATH</value>
                <description>
                        Expert: Additional execution environment entries for
                        map and reduce task processes. This is not an additive property.
                        You must preserve the original value if you want your map and
                        reduce tasks to have access to native libraries (compression, etc).
                        When this value is empty, the command to set execution
                        envrionment will be OS dependent:
                        For linux, use LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native.
                        For windows, use PATH = %PATH%;%HADOOP_COMMON_HOME%\\bin.
                </description>
        </property>

        <!-- DFSIO -->
        <property>
                <name>test.io.compression.class</name>
                <value>org.apache.hadoop.io.compress.SnappyCodec</value>
        </property>

        <property>
                <name>yarn.app.mapreduce.am.staging-dir</name>
                <value>/tmp/hadoop-yarn/staging</value>
                <description>The staging dir used while submitting jobs.
                </description>
        </property>

        <!-- log config -->
        <property>
                <name>mapreduce.task.userlog.limit.kb</name>
                <value>65536</value>
                <description>The maximum size of user-logs of each task in KB. 0 disables the cap.
                </description>
        </property>

        <property>
                <name>yarn.app.mapreduce.am.container.log.limit.kb</name>
                <value>32768</value>
                <description>The maximum size of the MRAppMaster attempt container logs in KB.
                        0 disables the cap.
                </description>
        </property>

        <property>
                <name>yarn.app.mapreduce.task.container.log.backups</name>
                <value>1</value>
                <description>Number of backup files for task logs when using
                        ContainerRollingLogAppender (CRLA). See
                        org.apache.log4j.RollingFileAppender.maxBackupIndex. By default,
                        ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA
                        is enabled for tasks when both mapreduce.task.userlog.limit.kb and
                        yarn.app.mapreduce.task.container.log.backups are greater than zero.
                </description>
        </property>

        <property>
                <name>yarn.app.mapreduce.am.container.log.backups</name>
                <value>1</value>
                <description>Number of backup files for the ApplicationMaster logs when using
                        ContainerRollingLogAppender (CRLA). See
                        org.apache.log4j.RollingFileAppender.maxBackupIndex. By default,
                        ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA
                        is enabled for the ApplicationMaster when both
                        mapreduce.task.userlog.limit.kb and
                        yarn.app.mapreduce.am.container.log.backups are greater than zero.
                </description>
        </property>

        <property>
                <name>mapreduce.job.userlog.retain.hours</name>
                <value>72</value>
                <description>The maximum time, in hours, for which the user-logs are to be
                        retained after the job completion.
                </description>
        </property>


        <!-- JobHistory Server ============================================================== -->
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>http://{% for host in groups['jobhistoryNodes'] %}{{ hostvars[host].ipv4_address }}{% if not loop.last %},{% endif %}{% endfor %}:10020</value>
                <description>MapReduce JobHistory Server IPC host:port</description>
        </property>

        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>http://{% for host in groups['jobhistoryNodes'] %}{{ hostvars[host].ipv4_address }}{% if not loop.last %},{% endif %}{% endfor %}:19888</value>
                <description>MapReduce JobHistory Server Web UI host:port</description>
        </property>

        <property>
                <name>mapreduce.jobhistory.intermediate-done-dir</name>
                <value>${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate</value>
                <description></description>
        </property>

        <property>
                <name>mapreduce.jobhistory.done-dir</name>
                <value>${yarn.app.mapreduce.am.staging-dir}/history/done</value>
                <description></description>
        </property>

        <property>
                <name>mapreduce.jobhistory.admin.address</name>
                <value>http://{% for host in groups['jobhistoryNodes'] %}{{ hostvars[host].ipv4_address }}{% if not loop.last %},{% endif %}{% endfor %}:10034</value>
                <description>The address of the History server admin interface.</description>
        </property>

        <property>
                <name>mapreduce.task.classpath.user.precedence</name>
                <value>false</value>
        </property>

        <property>
                <name>mapreduce.job.user.classpath.first</name>
                <value>false</value>
        </property>
</configuration>

